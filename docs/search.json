[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Website Introduction\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\nBen Townsend\n\n\n\n\n\n\n  \n\n\n\n\nCrop Type Mapping and Classification in Yolo County, CA\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nagriculture\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\nBen Townsend\n\n\n\n\n\n\n  \n\n\n\n\nSatellite Rainfall GIF\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nweather\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nBen Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Website Introduction",
    "section": "",
    "text": "Welcome to my github website! On this webpage, I will showcase and write about my different coding projects. Most of these posts will focus on geospatial data, geography, weather (specifically rainfall), and agriculture. I will mostly be using R and Python.\nThe creation of this website was inspired by a class I took this spring (2023) called Core Spatial Programming. In this class, we were required to make github website. I initially made a website from template that was primarily built on html code (with CSS and Javascript), but I decided to switch to a different website template (Quarto Blog), because it was a better fit for the type of posts I am planning to make.\nFor anyone who is interested, I have copied the my initial learning portfolio (which I wrote at the start of class in January 2023). I did not get to complete everything on the list, which I wasn’t expecting to. The ambitiousness of the list was to inspire and motivate me, and to give me ideas when I am having coder’s block (is that a phrase?)."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Research Interests\nAt a large scale, my primary research interest is the application of geospatial data and earth observation to human-environment issues, and how we can improve geospatial data and methods to improve human livelihoods. More specifically, I am interested in:\n\nThe application and advancement of geospatial data (particularly climate data) to environmental public health, both in the United States and globally.\nIdentifying gaps in satellite rainfall products (TAMSAT, CHIRPS, IMERG, etc.) and how we can improve these datasets for public health, disease monitoring, and food security.\nThe application and advancement of earth observation data for monitoring agrobiodiversity and agrobiodiversity change among smallholder farmers in South America, West Africa and Southern Africa.\n\n\n\nGraduate School Research\nMy current M.S research, as whole, focuses on how the unequal distribution of rain gauges impacts the accuracy of satellite rainfall products, using two different approaches: 1) who is left out in rain gauge networks (i.e: what is the profile of areas that do not have rain gauges, which are used to inform and validate satellite rainfall estimates) and 2) how does spatial autocorrelation impactthe accuracy of satellite rainfall validation, specifically in the context of merged gauges (gauges whose rainfall values are directly merged into satellite rainfall algorithms). My M.S project has three distinct goals:\n\nAnalyze how rain gauges vary across the different human and physical geographies of Ghana\n“Traditionally” validate four commonly used SRFEs using rainfall metrics relevant to malaria transmission\nExamine the role of spatial autocorrelation on satellite rainfall validation metrics using rain gauge networks where the autocorrelation range has been extended (e.g. excluding gauges within 50km of a merged gauge).\n\nOutside of my M.S research, I have conducted other research projects that have utilized my GIScience skills in graduate school:\n\nDuring my first semester of graduate school, I worked with another graduate student and a professor to investigate inequities in walkability in San Francisco County. This research resulted in a term paper that was presented at AAG in 2021 and is currently going through the publication process.\nIn the spring and summer of 2022, I worked on an independent project estimating the upper elevational limits of agriculture in the central Peruvian Andes, home to indigenous smallholder farmers. The project consisted of an extensive literature review on vegetation indices for crop identification, land cover analysis in mountainous regions, crop modeling, and socio-ecological systems. Unfortunately, due to data limitations, I was unable to complete the remote sensing/land cover analysis in full.\n\n\n\nUndergraduate Education\nI received a BA in Geography with concentration in Community and Global Health in 2020 from Macalester College in Saint Paul, MN. I also had an “honorary” minor in Portuguese (I was a few credits short of the official minor). While at Macalester, I was on the Varsity Cross Country and Track teams, and was the head writer my senior year for the Hegemonocle, Macalester’s humor magazine.\nI have been lucky to apply my GIS skills and other knowledge to a wide range of professional settings. During my sophomore year of college, I was a research and policy intern at Minnesota Brownfields, a non-profit whose mission is to promote brownfield cleanup and remediation for sustainable economic growth. My senior year of college, I was a student worker at the Minnesota Department of Health (MDH) in the Injury and Violence Prevention Unit. At MDH, my primary job was creating an excel database of police involved deadly force encounters from nationally recognized news outlets and crowdsourced databases. My work on this project was used to update MDH’s methodology for classifying police involved deadly force encounters.\n\n\nOther Interests\nOutside of academia and work, I enjoy running, road cycling, music, and sports, particularly basketball and baseball. I am hoping to showcase some of the sports analytics that I have done on this webpage."
  },
  {
    "objectID": "dataresources.html",
    "href": "dataresources.html",
    "title": "Data Resources",
    "section": "",
    "text": "This is a page I made to serve as a homebase for data resources (specifically geospatial data) for others and myself. The datasets and links below are ones that I have come across during classes, my own research, and through assisting Intro to GIS students with their own projects.\nIf there are any datasets that you think I should add, or that I unintentionally misrepresented, please let me know! As always, make sure you read the terms of use and give proper attribution when required!\n\nGeneral Resources\n\nEarth Engine Data Catalog: The Earth Engine Data Catalog showcases the wide range of data available for use in Google Earth Engine. There is a wide range of datasets, but a majority of the data is focused on the environment (vegetation, land cover, climate data). You can click on a dataset to learn more, and usually there is a link to the homepage of the dataset (in case you want to download the data to use in a non-GEE platform).\nState/Local Government Open Data: If you are working on a project about a specific location, the best place to start is that location’s open site. Some examples include City of Los Angeles, District of Columbia, and Wisconsin DNR\nUC-Irvine Machine Learning Repository: This is a good all round resource for practicing machine learning techniques.\n\n\n\nVector Based GIS Data\n\nCalEnviroScreen: This is one of my favorite datasets, as it’s great for a lot of different applications: statistical inference, geographically weighted regression, data visualization, and data manipulation. It comes in both vector and table format.\nNatural Earth: Great resource for creating reference maps.\nIPUMS: Includes historical census data for the United States and other countries, along with microsurveys on a wide range of topics. I believe you need an academic email to sign up.\n\n\n\nTerrestrial Data\n\nEarth Explorer and/or GloVis: These are two different websites/applications that can be used to download the suite of U.S. based satellite imagery, such as Landsat. You need an account to download data.\nCopernicus Open Access Hub: Website for downloading Sentinel-1 and Sentinel-2 data. You need an account to download data.\nPlanet: Provides free satellite imagery for students through their Research and Education Program Planet has some of the highest resolution imagery available at 3 m resolution (along with RapidEye data, at 5 m resolution).\nNASA Earth Data: Hosts a wide range of satellite data products, such as EVI and NDVI products. You need an account to download data.\n\n\n\nClimate/Weather Data\n\nIRI Climate Library: Has a wide range of climate and weather products available for download. Great source for downloading a large amount of climate data at once. There is a slight learning curve with navigating the website’s interface.\nTAMSAT: Satellite rainfall data for Africa. Data is available as far back as 1983, and the spatial resolution of the data is 4km. Data is available at the daily, pentad, dekad, monthly and seasonal levels.\nCHIRPS: Satellite rainfall data for anywhere on earth between 50 North and 50 South. Data is available as far back as 1981, and the spatial resolution of the data is roughly 5km.\n\n\n\nUseful Packages to collect data in R\n\ntidycensus: This is a great R package for census data. The author of the package also has a very thorough guide for using the package\nchirps: This is a useful R package for using CHIRPS data in R without having to download and store the data itself."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ben Townsend",
    "section": "",
    "text": "Hello! Thank you for visiting my website. I am currently a graduate student at Penn State, working towards a M.S in Geography. I will be graduating in May 2023. I’m interested in using GIScience and big data to solve and understand human-environment issues in the realm of public health, disease monitoring, and food security."
  },
  {
    "objectID": "posts/04-18-23-tamsatgif/index.html",
    "href": "posts/04-18-23-tamsatgif/index.html",
    "title": "Satellite Rainfall GIF",
    "section": "",
    "text": "Note: This is a project that created back in April 2023, on my previous website. Additionally, the full code for this project is available in a Github repository here.\n\nPart 1: Introduction\nSomething that I have wanted to do for a while now is a make an animation of monthly rainfall patterns in Ghana using satellite data. However, I had been hesitant to start, worried that netCDF files and gganimate would be complex and time extensive. Fortunately, both netCDF files and gganimate were pretty easy to use.\n\nPart 1A: Datasets\nThere were two datasets used in this project:\n\nMonthly Rainfall Data from TAMSAT, from January 1983 to December 2022\nShapefile of Ghana, used for cropping and masking the rainfall data\n\nThe monthly rainfall data from TAMSAT was downloaded as a .nc file from the IRI Climate Library. This includes all available months from January 1983 to December 2022.\n\n\nPart 1B: Packages\nBelow are a list of the packages I used for this project, and a brief explanation of the purpose of each package\n\nlibrary(raster) #For raster preprocessing\nlibrary(sf) #For loading the Ghana shapefile\nlibrary(ncdf4) #For opening rainfall files\nlibrary(tidyverse) #For any necessary data transformations and plotting\nlibrary(exactextractr) #For extracting pixel values\nlibrary(parallel) #For parallel computing\nlibrary(foreach) #For parallel computing\nlibrary(doParallel) #For parallel computing\nlibrary(tictoc) #For parallel computing\nlibrary(gganimate) #For creating gifs\n\n\n\nPart 1C: Functions\nI created two functions for this project to speed up some of the steps. The first function is called CropRaster, which streamlines the process of cropping raster data to the boundaries of a shapefile. It uses the crop and the mask functions from the raster package. The code for this function is below:\n\nCropRaster <- function(raster, boundary) {\n  raster.crop <- crop(raster, extent(boundary))\n  raster.mask <- mask(raster.crop, boundary)\n  return(raster.mask)\n}\n\nThe second function is RasterToDataframe, which streamlines the process of transforming a raster dataset into a dataframe. It uses the rasterToPoint function from the raster package to transform a raster in point data, and then uses the as.data.frame function to transform the point data into a dataframe. The resulting product is a dataframe with three variables: X Coordinate, Y Coordinate, and Value. The code for this function is below:\nRasterToDataframe <- function(raster) {\n  raster.pts <- rasterToPoints(raster)\n  raster.df <- as.data.frame(raster.pts)\n  return(raster.df)\n}\n\n\n\nPart 2: Image Processing\nTo ensure that the satellite rainfall was ready for animation, there were few preprocessing steps that occurred.\n\nPart 2a: Converting NetCDF files into Rasters\nThere are two steps for converting the NetCDF files into rasters. The first to extract the variables from the netCDF/nc files.\n\nlon <- ncvar_get(TAMSATMonth.nc, \"X\")\nlat <- ncvar_get(TAMSATMonth.nc, \"Y\")\nrainfall <- ncvar_get(TAMSATMonth.nc, \"rfe\")\n\nThere are three variables that we need to extract from the “TAMSATMonth.nc” file. We need to extract the location of what will become the pixels in the raster dataset, from the variables “X” and “Y”, which are longitude and latitude, respectively. We also need to extract the pixel value (a.k.a the rainfall amounts) for each month from the “rfe” (rain fall estimate) variable.\n\nfor(i in 1:480){\n  TAMSATMonth.raster[[i]] <- raster(rainfall[, , i], xmn=min(lat), \n                                    x = xmx=max(lat), \n                                    ymn=min(lon), ymx=max(lon), \n                                    crs=CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs+ towgs84=0,0,0\"))\n  TAMSATMonth.raster[[i]] <- t(TAMSATMonth.raster[[i]])\n  TAMSATMonth.raster[[i]] <- flip(TAMSATMonth.raster[[i]], 2)\n}\n\nThe next step is to transform the extracted variables into RasterLayers using the raster function from the raster package. In some ways, the “rainfall” object is already a raster, each entry represents the rainfall amount for a location, with rows serving as the x-coordinate and the columns representing the y-coordinates (for example rainfall[1, 1, 1], will give you the rainfall amount in the northwestern most pixel for January 1983). However, the rainfall object has no associated geographic reference with it, so we must use the “lon” and “lat” objects to define the extent of the raster data. We must also assign a coordinate reference system (crs) to the rasters as well, which is WGS 84. The last step is to transpose and flip the coordinates of raster, so that the pixels are in the correct location.\n\n\nPart 2b: Creating mean monthly rainfall rasters\nFor this part, we need to calculate the mean rainfall for each month. This step was a bit tricky. Our new raster, TAMSATMonth.raster, is a list of 480 rasters. In order to calculate the mean rainfall for each month, I had to add each raster to the mean function (typing out the name of the raster 40 times for each month). I’m not going to include the code here because it is kind of messy.\n\n\n\nPart 3: Creating the Animated Time Series\nThe final step in this project was to create the animation using gganimate. To do so however, requires transforming the raster data into a dataframe using the RasterToDataFrame function from above.\nOnce a dataframe for each month has been created, a new column called “Month” is created for every dataframe, with the value of the dataframe corresponding to the month that is represented by the dataframe. Next, the rbind function is used to combine the 12 dataframes together into one. The last step was to use the factor function to order the observations by month, so that the months are displayed in the correct order in the animation.\n\nPart 3A: gganimate\nNow that all of the data preprocessing is done, we can make an animation of rainfall in Ghana. Below is the code used to make the animation:\n\np <- ggplot() +\n  geom_sf(data = Ghana) +\n  geom_raster(data = MonthlyMean, aes(x = x, y = y, fill = layer)) +\n  scale_fill_gradientn(colours=c(\"#001137\", \"#0aab1e\", \"#e7eb05\", \"#ff4a2d\", \"#e90000\"),\n                       name=\"Rainfall (mm)\") +\n  labs(title = 'Month: {closest_state}', x = \"Longitude\", y = \"Latitude\") +\n  transition_states(Month, transition_length = 1, state_length = 2)\n\nThe code for the animation looks very similar to normal ggplot until you get to the fifth argument (line 6), the argument “title” has {closest_state}. I’ll get to this in a moment, it will make more sense to explain what is going on in line 7 first.\nIn line 7, there is a function called transition_states, that determines the parameters of the animation. There are three required arguments:\n\nstates: This is the column to be animated. “states” refers to the categories displayed in the column, which in this case in months. The states argument is similar to the facet_wrap or color arguements in ggplot.\ntransition_length: Refers to the length of the transition in the animation. This is a relative amount, based on the number of states in the dataset.\nstate_length: Refers to the length of the states in the animation. This is a relative amount, based on the number of stats in the dataset.\n\nThe transition_length and state_length arguments can be a bit obscure; my recommendation is that you plug and play the numbers you into the arguments, but make sure that the transition_length argument is less than the state_length argument.Back to line 6 quickly - in the argument “title”, {closest_state} will show the name of the month that is closest to the current frame.\nTo actually display the animation, we need to save our ggplot as an object (“p”), and then use the animate function to display the animation.\n\nanimate(p)\n\nThe animate function might take a minute to load, but once it is done, the animation will be displayed in your viewer. When the animation is to your liking, you can use the save_animation function to save your animation as a gif, with the first argument being the name you wish to give the file, and the second argument being the animation you want to save.\n\nsave_animation(\"MeanRainfallTAMSAT.gif\", animation = last_animation())\n\nBelow is the final product:\n\n\n\n\n\nThe animation does a good job at showing the general seasonal rainfall trends in Ghana. The animation, however, is a bit a clunky when it hits July, but this is because there is a small dry season in southern Ghana in July and August.\n\n\n\nNext Steps/Future Directions\nIf were to build upon this project in the future, below are a few things I would consider doing the following:\n*Tweak the parameters of the animation frame length to create a smoother transition\n*Change the temporal extent and resolution of the rainfall (ex. do daily rainfall for the month of May.)\n*Try to visualize a single storm rather than long term rainfall patterns.\n*Find a more efficient method to parse out the months in the list of rasters"
  },
  {
    "objectID": "posts/welcome/index.html#geog-560-learning-portfolio",
    "href": "posts/welcome/index.html#geog-560-learning-portfolio",
    "title": "Website Introduction",
    "section": "GEOG 560 Learning Portfolio",
    "text": "GEOG 560 Learning Portfolio\nThis semester (Spring 2023), I am taking a class called Core Spatial Programming, which focuses on improving the programming skills of geographers. For the class, we are making learning/coding/programming portfolios to display our programming skills and to show how our programming skills have improved over the semester. I have decided to host my portfolio here. Below for my learning goals for this class (NOTE: I do not intend to accomplish all of the learning goals listed on this website in one semester, I expect to continue to grow this website over time).\n\nLearning Goals\nBelow are a list of coding skills that I wanted to improve on over the semester (and in the future!). I’ve organized them by programming langauge. For reference my skill level in R is somewhere between intermediate and advanced, my skill level in Python is somewhere between beginnner and intermediate.\n\nR Programming\n\nImprove the efficiency of my code\n\nLength - how can I shorten my scripts to use less code and less objects?\nTime - how can I make my code run more efficiently?\n\nImprove the organization of my scripts\n\nWhat are best practices for making code easy to read, both for myself and for collaborators?\nHow do I organize the contents of my scripts, so that from a content standpoint, they are logically put together?\nHow do I create a “ladder”/workflow of scripts that is flexible?\n\nCreate shiny apps/web apps for the interactive display of rainfall data, environmental data, and basketball data\nCreate packages that perform functions relevant to geospatial data, rainfall, etc.\nLearn how to do machine learning, clustering and neural networks (if possible)\n\n\n\nPython Programming\n\nLearn how to properly set up a python environment on my laptop for a desktop application and jupyter notebooks (So that it complements google colab)\nImprove my Python skills so that they are at a similar level as my R skills\n\nFeel comfortable doing data manipulation\nFeel comfortable displaying and manipulating geospatial data\nFeel comfortable with creating visualizations\nHave a solid understanding of key packages (both geospatial and non-geospatial)\n\nBe able to apply the skills I learned in my machine learning class to my M.S research and research interests\n\n\n\nJavascript (if time)\nIf there is time, I would also like to spend time learning JavaScript, for geospatial applications, such as:\n\nGoogle Earth Engine (I have some experience with GEE)\nD3, to create interactive maps (I do not have any existing knowledge on D3)"
  },
  {
    "objectID": "posts/04-25-23-yolocrops/index.html",
    "href": "posts/04-25-23-yolocrops/index.html",
    "title": "Crop Type Mapping and Classification in Yolo County, CA",
    "section": "",
    "text": "Note: This is a project that created back in April 2023, on my previous website. Additionally, the full code for this project is available in a Github repository here.\n\nPart 1: Introduction\nOne of the research projects I worked on early during my time in graduate school was to analyze the upper elevation limit of agriculture and agrobiodiversity change in the Peruvian Andes. For this project, I did an extensive literature review of on the remote sensing of agriculture and using machine learning techniques to classify crops by species. Unfortunately, I was unable to complete the remote sensing/machine learning portion of the project, due to unforeseen data limitations.For a while now, I have wanted to apply my skills I have learned from this experience.\nFor this project, I used Random Forests and Support Vector Machine algorithms to classify four different crops in Yolo County, CA, which is just west of Sacramento. I used both R and Python for this project: R was used to clean the data, calculate vegetation indices, and to extract pixel values to the crop fields. Python (run in google colab) was used for machine learning.\n\nPart 1A: Datasets\nThere were two datasets that were used in this project:\n\nLevel 2A Sentinel-2 data of Yolo County, from June 28th, 2020\nA shapefile of Crops in Yolo County, CA for 2020 (from Esri Open Data/Yolo County)\n\n\n\nPart 1B: Packages\nBelow are a list of the R packages I used for this project, and a brief explanation of how the package was used in this project.\n\nlibrary(raster) #Uploading sentinel data\nlibrary(sf) #Uploading crop shapefile\nlibrary(exactextractr) #For extracting pixel values in crop fields\nlibrary(tidyverse) #For filtering fields by crop type\nlibrary(parallel) #For parallel processing\nlibrary(foreach) #For parallel processing\nlibrary(doParallel) #For parallel processing\nlibrary(tictoc) #For parallel processing\n\nBelow are a list of the Python packages I used for this project, and a brief explanation of how the package was used in this project.\n\nimport pandas as pd #For Data Manipulation\nimport seaborn as sns #For plotting confusion matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC #For Running SVM algorithm\nfrom sklearn.metrics import confusion_matrix #For creating confusion matrix\nfrom sklearn.model_selection import train_test_split #For splitting data\nfrom sklearn import preprocessing #For normalizing data (SVM Only)\n\n\n\nPart 1C: Functions (Vegetation Indices)\nFor this project, I created functions to calculate five vegetation indices that are present in the agricultural remote sensing literature:\n\n#Enhanced Vegetation Index (EVI)\nEVI <- function(B2, B4, B8) {\n  EVIscore <- 2.5 * ((B8 - B4) / (B8 + 6 * B4 - 7.5 * B2 + 1))\n  return(EVIscore)\n}\n\n\n#Green Chlorophyll Index (GCI)\nGCI <- function(B3, B8) {\n  GCIscore <- (B8/B3) - 1\n  return(GCIscore)\n}\n\n\n#Plant Senescence Reflectance Index (PSRI)\nPSRI <- function(B2, B4, B6) {\n  PSRIscore <- (B4-B2) / B6\n  return(PSRIscore)\n}\n\n\n#Soil-Adjusted Vegetation Index (SAVI)\nSAVI <- function(B4, B8) {\n  SAVIscore <- 1.5 * (B8 - B4) / (B8 + B4 + 0.5)\n  return(SAVIscore)\n}\n\n\nNDTI <- function(B11, B12) {\n  NDTIscore <- (B11 - B12) / (B11 + B12)\n  return(NDTIscore)\n}\n\n\n\n\nPart 2: Data Preparation\nThe end goal of the data preparation section in R is to create a csv file of each walnut, almond, grape (wine) and olive field with following columns:\n\nCrop Type of the field (walnut, almond, grape or olive) (1 column)\nThe mean reflectance value for all pixels in a crop field for Band 2 (Blue), Band 3 (Green), Band 4 (Red), Band 5 (Red-Edge), Band 6 (Red-Edge), Band 7 (Red-Edge), Band 8 (NIR), Band 8a (NIR), Band 11 (SWIR), Band 12 (SWIR)\nThe mean pixel value of five vegetation indices (EVI, GCI, PSRI, SAVI, and NDTI). These bands and indices will all be inputs into the machine learning model.\n\n\nPart 2A: Image Processing\nThere are two steps in this part. First, the bands from the satellite images must be divided by 10000 to get the reflectance values (the bands are not automatically at reflectance value due to storage and technical reasons). The code is pretty straightforward\n\nfor (i in 1:4){\n  YoloCoImages10m[[i]] <-YoloCoImages10m[[i]] / 10000\n}\nfor (i in 1:8){\n  YoloCoImages20m[[i]] <- YoloCoImages20m[[i]] / 10000\n}\n\nThe next step is to create raster layers of the different vegetation indices. We do this using the functions created in part 1C.\n\n\nPart 2B: Pixel Extraction\nIn this part, we use the exact_extract function to extract the mean pixel value of the crop fields, and add them as a column to the crop shapefile. An example of the code is below, where “EVIYoloCo” is the raster layer, “Crops”, is the shapefile, and “mean” is the statistic used:\n\nCrops$EVI <- exact_extract(EVIYoloCo, Crops, \"mean\")\n\n\n\nPart 2C: Filtering Fields by Crop Type\nThe next part is to filter the crops dataset to include only the fields that are walnut, almond, grape (wine) or olive crops. These crops were selected because they had the highest number of observations in the dataset. This can be done using dplyr:\n\nCrops <- Crops %>% filter(croptype == \"OLIVE\" | croptype == \"WALNUT\" | \n                          croptype == \"ALMOND\" | croptype == \"GRAPE, WINE\")\n\nNote: This part can be done before extracting the pixel values, it depends on personal preference.\n\n\n\nPart 3: Machine Learning Classification\nI ran two different machine learning models for classifying the crops, using the sklearn package in python: random forests and support vector machine (SVM). For the random forest model, the number of trees was 80. For the SVM model, I used a linear kernel and C parameter of 15. For both models, the train/test split was 70/30.\n\nPart 3A: Random Forests\nBelow is the code used for the running the random forests model:\n\n# Split data into training and test datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n#Build Random Forest Classifier\nforest = RandomForestClassifier(n_estimators=80, n_jobs=-1, \n                                random_state=42)\nforest.fit(X_train, Y_train)\nY_predRF = forest.predict(X_test)\n#Get Scores\nprint('Training set score: {:.4f}'.format(forest.score(X_train, Y_train)))\nprint('Test set score: {:.4f}'.format(forest.score(X_test, Y_test)))\n\nThe random forests classifier was very accurate, as the accuracy score for the training dataset was 100%, while the accuracy score the test dataset was 87%. However, the 13% difference between the training and test scores suggests overfitting. We can take a closer look into the classification of the crops using a confusion matrix:\n\n#Create confusion matrix for Random Forests\nfrom sklearn.metrics import confusion_matrix\nconfusionmatrix=confusion_matrix(Y_test, Y_predRF)\n\ncm_matrix = pd.DataFrame(data=confusionmatrix, columns=['Predict Almond', 'Predict Grape', 'Predict Olive', 'Predict Walnut'], \n                                 index=['Almond', 'Grape', 'Olive', 'Walnut'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n\n\n\n\nCaption\n\n\nAbove is a confusion matrix for the test datasets and its predictions for the random forests model. The model was accurate 92.6% of the time for almonds, 86.6% of the time for grapes, 68.4% of the time for olives, and 82.5% of the time for walnuts. The most common misclassification was walnuts being classified as almonds (12 times)\n\n\nPart 3B: Support Vector Machine\nThere is one additional preprocessing steps that needs to take place before running the SVM model, which is to normalize the data:\n\n#Normalize the input data\nX_norm=preprocessing.scale(X)\n\nNow we can run the SVM model. The code for the SVM model is below:\n\n#Split the data into train/test data\nX_train, X_test, Y_train, Y_test = train_test_split(X_norm, y, test_size=0.3, random_state=0)\n\n#Set kernel type to linear\nlinear_svc=SVC(kernel='linear', C=15.0, probability=True) \n\n#Fit the data\nlinear_svc.fit(X_train,Y_train)\n\n#Make Predictions\nY_predSVML = linear_svc.predict(X_test)\n\n# Print Scores\nprint('Training set score: {:.4f}'.format(linear_svc.score(X_train, Y_train)))\nprint('Test set score: {:.4f}'.format(linear_svc.score(X_test, Y_test)))\n\nThe SVM classifier was less accurate than the random forests classifier, as the accuracy score for the training dataset was 88%, while the accuracy score the test dataset was 84%. However, the SVM model had less overfitting than the random forests model, with a 4% difference between the training and test scores. We can take a closer look into the classification of the crops using a confusion matrix:\n\n#Create confusion matrix for Random Forests\nfrom sklearn.metrics import confusion_matrix\nconfusionmatrix=confusion_matrix(Y_test, Y_predRF)\n\ncm_matrix = pd.DataFrame(data=confusionmatrix, columns=['Predict Almond', 'Predict Grape', 'Predict Olive', 'Predict Walnut'], \n                                 index=['Almond', 'Grape', 'Olive', 'Walnut'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n\n Above is a confusion matrix for the test datasets and its predictions for the SVM model. The model was accurate 90.9% of the time for almonds, 80.5% of the time for grapes, 63.2% of the time for olives, and 82.5% of the time for walnuts. The most common misclassification was walnut crops being classified as almond crops (16 times).\n\n\n\nPart 4: Next Steps\nIn future, some improvements I would make to the project include:\nExpand the number of crops being classified. Spend more time tuning the hyperparameters for the machine learning models. Use neural networks to classify crop type. Try to run the entire project in one language (R or Python)."
  }
]