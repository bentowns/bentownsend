{
  "hash": "83fac63b50f9b16c3224d427286cb1ac",
  "result": {
    "markdown": "---\ntitle: \"Crop Type Mapping and Classification in Yolo County, CA\"\nauthor: \"Ben Townsend\"\ndate: \"2023-04-25\"\ncategories: [code, machine learning, agriculture]\nexecute: \n  echo: true\n  eval: false\nimage: \"YoloCoSAVI.png\"\n---\n\n\n*Note: This is a project that created back in April 2023, on my previous website. Additionally, the full code for this project is available in a Github repository [here](https://github.com/bentowns/YoloCountyCrops).*\n\n### **Part 1: Introduction**\n\nOne of the research projects I worked on early during my time in graduate school was to analyze the upper elevation limit of agriculture and agrobiodiversity change in the Peruvian Andes. For this project, I did an extensive literature review on the remote sensing of agriculture and using machine learning techniques to classify crops by species. Unfortunately, I was unable to complete the remote sensing/machine learning portion of the project, due to unforeseen data limitations. For a while now, I have wanted to apply my skills I have learned from this experience.\n\nFor this project, I used Random Forests and Support Vector Machine algorithms to classify four different crops in Yolo County, CA, which is just west of Sacramento. I used both R and Python for this project: R was used to clean the data, calculate vegetation indices, and to extract pixel values to the crop fields. Python (run in google colab) was used for running the machine learning algorithms.\n\n#### **Part 1A: Datasets**\n\nTwo datasets that were used in this project:\n\n-   Level 2A Sentinel-2 data of Yolo County, from June 28th, 2020\n-   A shapefile of Crops in Yolo County, CA for 2020 (from Esri Open Data/Yolo County)\n\n#### **Part 1B: Packages**\n\nBelow are a list of the R packages I used for this project, and a brief explanation of how the package was used in this project:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(raster) #Uploading sentinel data\nlibrary(sf) #Uploading crop shapefile\nlibrary(exactextractr) #For extracting pixel values in crop fields\nlibrary(tidyverse) #For filtering fields by crop type\nlibrary(parallel) #For parallel processing\nlibrary(foreach) #For parallel processing\nlibrary(doParallel) #For parallel processing\nlibrary(tictoc) #For parallel processing\nlibrary(reticulate) #For python compatability\n```\n:::\n\n\nBelow are a list of the Python packages I used for this project, and a brief explanation of how the package was used in this project:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd #For Data Manipulation\nimport seaborn as sns #For plotting confusion matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC #For Running SVM algorithm\nfrom sklearn.metrics import confusion_matrix #For creating confusion matrix\nfrom sklearn.model_selection import train_test_split #For splitting data\nfrom sklearn import preprocessing #For normalizing data (SVM Only)\n```\n:::\n\n\n#### **Part 1C: Functions (Vegetation Indices)**\n\nFor this project, I created functions to calculate five vegetation indices that are present in the literature of the remote sensing of agriculture:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Enhanced Vegetation Index (EVI)\nEVI <- function(B2, B4, B8) {\n  EVIscore <- 2.5 * ((B8 - B4) / (B8 + 6 * B4 - 7.5 * B2 + 1))\n  return(EVIscore)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Green Chlorophyll Index (GCI)\nGCI <- function(B3, B8) {\n  GCIscore <- (B8/B3) - 1\n  return(GCIscore)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plant Senescence Reflectance Index (PSRI)\nPSRI <- function(B2, B4, B6) {\n  PSRIscore <- (B4-B2) / B6\n  return(PSRIscore)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Soil-Adjusted Vegetation Index (SAVI)\nSAVI <- function(B4, B8) {\n  SAVIscore <- 1.5 * (B8 - B4) / (B8 + B4 + 0.5)\n  return(SAVIscore)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nNDTI <- function(B11, B12) {\n  NDTIscore <- (B11 - B12) / (B11 + B12)\n  return(NDTIscore)\n}\n```\n:::\n\n\n### **Part 2: Data Preparation**\n\nThe end goal of the data preparation section in R is to create a csv file of each walnut, almond, grape (wine) and olive field with the following columns:\n\n-   Crop Type of the field (walnut, almond, grape or olive) (1 column)\n-   The mean reflectance value for all pixels in a crop field for Band 2 (Blue), Band 3 (Green), Band 4 (Red), Band 5 (Red-Edge), Band 6 (Red-Edge), Band 7 (Red-Edge), Band 8 (NIR), Band 8a (NIR), Band 11 (SWIR), Band 12 (SWIR)\n-   The mean pixel value of five vegetation indices (EVI, GCI, PSRI, SAVI, and NDTI). These bands and indices will all be inputs into the machine learning model.\n\n#### **Part 2A: Image Processing**\n\nThere are two steps in this part. First, the bands from the satellite images must be divided by 10000 to get the reflectance values (the bands are not automatically at reflectance value due to storage and technical reasons).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in 1:4){\n  YoloCoImages10m[[i]] <-YoloCoImages10m[[i]] / 10000\n}\nfor (i in 1:8){\n  YoloCoImages20m[[i]] <- YoloCoImages20m[[i]] / 10000\n}\n```\n:::\n\n\nThe next step is to create raster layers of the different vegetation indices. We do this using the functions created in part 1C.\n\n#### **Part 2B: Pixel Extraction**\n\nIn this part, we use the exact_extract function to extract the mean pixel value of the crop fields, and add them as a column to the crop shapefile. An example of the code is below, where \"EVIYoloCo\" is the raster layer, \"Crops\", is the shapefile, and \"mean\" is the statistic used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCrops$EVI <- exact_extract(EVIYoloCo, Crops, \"mean\")\n```\n:::\n\n\n#### **Part 2C: Filtering Fields by Crop Type**\n\nThe next part is to filter the crops dataset to include only the fields that are walnut, almond, grape (wine) or olive crops. These crops were selected because they had the highest number of observations in the dataset. Filtering was done using dplyr:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCrops <- Crops %>% filter(croptype == \"OLIVE\" | croptype == \"WALNUT\" | \n                          croptype == \"ALMOND\" | croptype == \"GRAPE, WINE\")\n```\n:::\n\n\nNote: This part (2C) can be done before extracting the pixel values, it depends on personal preference.\n\n### **Part 3: Machine Learning Classification**\n\nI ran two different machine learning models for classifying the crops, using the sklearn package in python: random forests and linear support vector machine (SVM). For the random forest model, the number of trees was 80. For the SVM model, I used a linear kernel and C parameter of 15. For both models, the train/test split was 70/30.\n\n#### **Part 3A: Random Forests**\n\nBelow is the code used for the running the random forests model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Split data into training and test datasets\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n#Build Random Forest Classifier\nforest = RandomForestClassifier(n_estimators=80, n_jobs=-1, \n                                random_state=42)\nforest.fit(X_train, Y_train)\nY_predRF = forest.predict(X_test)\n#Get Scores\nprint('Training set score: {:.4f}'.format(forest.score(X_train, Y_train)))\nprint('Test set score: {:.4f}'.format(forest.score(X_test, Y_test)))\n```\n:::\n\n\nThe random forests classifier was very accurate, as the accuracy score for the training dataset was 100%, while the accuracy score the test dataset was 87%. However, the 13% difference between the training and test datasets suggests overfitting. We can take a closer look into the classification of the crops using a confusion matrix:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Create confusion matrix for Random Forests\nfrom sklearn.metrics import confusion_matrix\nconfusionmatrix=confusion_matrix(Y_test, Y_predRF)\n\ncm_matrix = pd.DataFrame(data=confusionmatrix, columns=['Predict Almond', 'Predict Grape', 'Predict Olive', 'Predict Walnut'], \n                                 index=['Almond', 'Grape', 'Olive', 'Walnut'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n```\n:::\n\n\n![Caption](YoloCropsRF.png) Above is a confusion matrix for the test dataset and its predictions for the random forests model. The model was accurate 92.6% of the time for almonds, 86.6% of the time for grapes,68.4% of the time for olives, and 82.5% of the time for walnuts. The most common misclassification was walnuts being classified as almonds (12 times).\n\n#### **Part 3B: Support Vector Machine**\n\nThere is one additional preprocessing step that needs to take place before running the SVM model, which is to normalize the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Normalize the input data\nX_norm=preprocessing.scale(X)\n```\n:::\n\n\nNow we can run the SVM model. The code for the SVM model is below:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Split the data into train/test data\nX_train, X_test, Y_train, Y_test = train_test_split(X_norm, y, test_size=0.3, random_state=0)\n\n#Set kernel type to linear\nlinear_svc=SVC(kernel='linear', C=15.0, probability=True) \n\n#Fit the data\nlinear_svc.fit(X_train,Y_train)\n\n#Make Predictions\nY_predSVML = linear_svc.predict(X_test)\n\n# Print Scores\nprint('Training set score: {:.4f}'.format(linear_svc.score(X_train, Y_train)))\nprint('Test set score: {:.4f}'.format(linear_svc.score(X_test, Y_test)))\n```\n:::\n\n\nThe SVM classifier was less accurate than the random forests classifier, as the accuracy score for the training dataset was 88%, while the accuracy score for the test dataset was 84%. However, the SVM model had less overfitting than the random forests model, with a 4% difference between the training and test scores. We can take a closer look into the classification of the crops using a confusion matrix:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#Create confusion matrix for Random Forests\nfrom sklearn.metrics import confusion_matrix\nconfusionmatrix=confusion_matrix(Y_test, Y_predRF)\n\ncm_matrix = pd.DataFrame(data=confusionmatrix, columns=['Predict Almond', 'Predict Grape', 'Predict Olive', 'Predict Walnut'], \n                                 index=['Almond', 'Grape', 'Olive', 'Walnut'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n```\n:::\n\n\n![Caption](YoloCropsSVM.png) Above is a confusion matrix for the test datasets and its predictions for the SVM model. The model was accurate 90.9% of the time for almonds, 80.5% of the time for grapes, 63.2% of the time for olives, and 82.5% of the time for walnuts. The most common misclassification was walnut crops being classified as almond crops (16 times).\n\n### **Part 4: Next Steps**\n\nIn future, some improvements I would make to the project include:\n\n-   Expand the number of crops being classified.\n-   Spend more time tuning the hyper-parameters for the machine learning models.\n-   Use neural networks to classify crop type.\n-   Try to run the entire project in one language (R or Python).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}